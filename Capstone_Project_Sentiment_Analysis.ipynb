{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amazon Prodcut Sentiment Analysis:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goals: \n",
    "    \n",
    "    1. Perform sentiment analysis using the following techniques:\n",
    "        a. Logistic Regression with TFIDF vectorizer\n",
    "        b. Logistic Regression with TFIDF vectorizer and n-grams techniques\n",
    "        c. SVM classifier with TFIDF vectorizer and n-grams techniques\n",
    "        d. Naive Bayes Classifier with TFIDF vectorizer and n-grams techniques\n",
    "    2. Analyzing the performance metrics of each of those above models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step1: Importing required packages and loading the required dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import *\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from string import punctuation\n",
    "from sklearn import svm\n",
    "from textblob import Word\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from wordcloud import WordCloud\n",
    "import textblob\n",
    "from textblob import TextBlob\n",
    "#from imblearn.pipeline import Pipeline as imbPipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have used 5-core Home and Kitchen reviews dataset. It is subset of the data in which all users and items have at least 5 reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "\n",
    "def parse(path):\n",
    "  g = gzip.open(path, 'rb')\n",
    "  for l in g:\n",
    "    yield eval(l)\n",
    "\n",
    "def getDF(path):\n",
    "  i = 0\n",
    "  df = {}\n",
    "  for d in parse(path):\n",
    "    df[i] = d\n",
    "    i += 1\n",
    "  return pd.DataFrame.from_dict(df, orient='index')\n",
    "\n",
    "df = getDF('reviews_Home_and_Kitchen_5.json.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data processing for further analytics and modeling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "reviewerID           0\n",
       "asin                 0\n",
       "reviewerName      4953\n",
       "helpful              0\n",
       "reviewText           0\n",
       "overall              0\n",
       "summary              0\n",
       "unixReviewTime       0\n",
       "reviewTime           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no any missing values in any variable except that in reviewerName variable. We do not need reviewerName variable in our analysis.  So, We will continue using all data for our further analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying to understand the customer sentiment in multiple directions. I would like to start with Logistic Regression and will see how it works. \n",
    "1. Feature extraction/Feature engineering. In this process let's see how we can use 'helpful' variable to understand customer preferences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make new two columns out of 'helpful' column\n",
    "df[['helpfulfirstelement', 'helpfulsecondelement']] = pd.DataFrame(df.helpful.values.tolist(), index = df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove any duplicates\n",
    "df = df.drop_duplicates(subset= ['reviewerID', 'asin', 'unixReviewTime'], keep = 'first')\n",
    "# Adding helpful percentage and upvote\n",
    "df['helpfulPercent'] = np.where(df['helpfulsecondelement'] > 0, df['helpfulfirstelement']/df['helpfulsecondelement'], -1)\n",
    "df['upvotePercent'] = pd.cut(df['helpfulPercent'], bins = [-1, 0, 0.2, 0.4, 0.6, 0.8, 1.0], labels = ['Empty', '0-20%', '20-40%', '40-60%', '60-80%', '80-100%'], include_lowest=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = df.groupby(['overall', 'upvotePercent']).agg({'overall': 'count'}).rename(columns = {'overall': 'reviews_count'}).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I am planning to use reviews text and overall score columns for my further modeling. Here for simplicity I am planning to take two classes by neglecting the overall rating of 3. I will classify overall rating score of [1 and 2] as 0 and overall rating score of [4 and 5] as 1. Finally, I will use reviews text and predict the overall score of either 0 or 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model = df[df['overall'] != 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>reviewerID</th>\n",
       "      <td>APYOBQE6M18AA</td>\n",
       "      <td>A1JVQTAGHYOL7F</td>\n",
       "      <td>A3UPYGJKZ0XTU4</td>\n",
       "      <td>A2MHCTX43MIMDZ</td>\n",
       "      <td>AHAI85T5C2DH3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>asin</th>\n",
       "      <td>0615391206</td>\n",
       "      <td>0615391206</td>\n",
       "      <td>0615391206</td>\n",
       "      <td>0615391206</td>\n",
       "      <td>0615391206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reviewerName</th>\n",
       "      <td>Martin Schwartz</td>\n",
       "      <td>Michelle Dinh</td>\n",
       "      <td>mirasreviews</td>\n",
       "      <td>M. Johnson \"Tea Lover\"</td>\n",
       "      <td>PugLover</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>helpful</th>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>[26, 27]</td>\n",
       "      <td>[14, 18]</td>\n",
       "      <td>[0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reviewText</th>\n",
       "      <td>My daughter wanted this book and the price on ...</td>\n",
       "      <td>I bought this zoku quick pop for my daughterr ...</td>\n",
       "      <td>There is no shortage of pop recipes available ...</td>\n",
       "      <td>This book is a must have if you get a Zoku (wh...</td>\n",
       "      <td>This cookbook is great.  I have really enjoyed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>overall</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>summary</th>\n",
       "      <td>Best Price</td>\n",
       "      <td>zoku</td>\n",
       "      <td>Excels at Sweet Dessert Pops, but Falls Short ...</td>\n",
       "      <td>Creative Combos</td>\n",
       "      <td>A must own if you own the Zoku maker...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unixReviewTime</th>\n",
       "      <td>1382140800</td>\n",
       "      <td>1403049600</td>\n",
       "      <td>1367712000</td>\n",
       "      <td>1312416000</td>\n",
       "      <td>1402099200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reviewTime</th>\n",
       "      <td>10 19, 2013</td>\n",
       "      <td>06 18, 2014</td>\n",
       "      <td>05 5, 2013</td>\n",
       "      <td>08 4, 2011</td>\n",
       "      <td>06 7, 2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>helpfulfirstelement</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>helpfulsecondelement</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>helpfulPercent</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.962963</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>upvotePercent</th>\n",
       "      <td>Empty</td>\n",
       "      <td>Empty</td>\n",
       "      <td>80-100%</td>\n",
       "      <td>60-80%</td>\n",
       "      <td>Empty</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                      0  \\\n",
       "reviewerID                                                APYOBQE6M18AA   \n",
       "asin                                                         0615391206   \n",
       "reviewerName                                            Martin Schwartz   \n",
       "helpful                                                          [0, 0]   \n",
       "reviewText            My daughter wanted this book and the price on ...   \n",
       "overall                                                               5   \n",
       "summary                                                      Best Price   \n",
       "unixReviewTime                                               1382140800   \n",
       "reviewTime                                                  10 19, 2013   \n",
       "helpfulfirstelement                                                   0   \n",
       "helpfulsecondelement                                                  0   \n",
       "helpfulPercent                                                       -1   \n",
       "upvotePercent                                                     Empty   \n",
       "\n",
       "                                                                      1  \\\n",
       "reviewerID                                               A1JVQTAGHYOL7F   \n",
       "asin                                                         0615391206   \n",
       "reviewerName                                              Michelle Dinh   \n",
       "helpful                                                          [0, 0]   \n",
       "reviewText            I bought this zoku quick pop for my daughterr ...   \n",
       "overall                                                               5   \n",
       "summary                                                            zoku   \n",
       "unixReviewTime                                               1403049600   \n",
       "reviewTime                                                  06 18, 2014   \n",
       "helpfulfirstelement                                                   0   \n",
       "helpfulsecondelement                                                  0   \n",
       "helpfulPercent                                                       -1   \n",
       "upvotePercent                                                     Empty   \n",
       "\n",
       "                                                                      2  \\\n",
       "reviewerID                                               A3UPYGJKZ0XTU4   \n",
       "asin                                                         0615391206   \n",
       "reviewerName                                               mirasreviews   \n",
       "helpful                                                        [26, 27]   \n",
       "reviewText            There is no shortage of pop recipes available ...   \n",
       "overall                                                               4   \n",
       "summary               Excels at Sweet Dessert Pops, but Falls Short ...   \n",
       "unixReviewTime                                               1367712000   \n",
       "reviewTime                                                   05 5, 2013   \n",
       "helpfulfirstelement                                                  26   \n",
       "helpfulsecondelement                                                 27   \n",
       "helpfulPercent                                                 0.962963   \n",
       "upvotePercent                                                   80-100%   \n",
       "\n",
       "                                                                      3  \\\n",
       "reviewerID                                               A2MHCTX43MIMDZ   \n",
       "asin                                                         0615391206   \n",
       "reviewerName                                     M. Johnson \"Tea Lover\"   \n",
       "helpful                                                        [14, 18]   \n",
       "reviewText            This book is a must have if you get a Zoku (wh...   \n",
       "overall                                                               5   \n",
       "summary                                                 Creative Combos   \n",
       "unixReviewTime                                               1312416000   \n",
       "reviewTime                                                   08 4, 2011   \n",
       "helpfulfirstelement                                                  14   \n",
       "helpfulsecondelement                                                 18   \n",
       "helpfulPercent                                                 0.777778   \n",
       "upvotePercent                                                    60-80%   \n",
       "\n",
       "                                                                      4  \n",
       "reviewerID                                                AHAI85T5C2DH3  \n",
       "asin                                                         0615391206  \n",
       "reviewerName                                                   PugLover  \n",
       "helpful                                                          [0, 0]  \n",
       "reviewText            This cookbook is great.  I have really enjoyed...  \n",
       "overall                                                               4  \n",
       "summary                         A must own if you own the Zoku maker...  \n",
       "unixReviewTime                                               1402099200  \n",
       "reviewTime                                                   06 7, 2014  \n",
       "helpfulfirstelement                                                   0  \n",
       "helpfulsecondelement                                                  0  \n",
       "helpfulPercent                                                       -1  \n",
       "upvotePercent                                                     Empty  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_model.head(5).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_model['reviewText']\n",
    "y = df_model['overall'].map({1:0, 2:0, 4:1, 5:1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initially, I will start with applying simple logistic regression to predict whether the rating is either positive (1) or negative (0)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentiment analysis:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's detect the sentiment of the review. Before applying any machine learning models, ,let's check the sentiment of the first few reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                 (0.9, 0.65)\n",
       "1                  (0.3916666666666666, 0.55)\n",
       "2     (0.2908062770562771, 0.572667748917749)\n",
       "3    (0.3873809523809524, 0.7209523809523809)\n",
       "4    (0.2822222222222222, 0.6692592592592592)\n",
       "Name: reviewText, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['reviewText'][:5].apply(lambda x: TextBlob(x).sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It returns a tuple representing polarity and subjectivity of each review. Here I only extract polarity as it indicates the sentiment as value nearer to 1 means positive sentiment and values nearer to -1 means a negative sentiment. This can also work as a feature for building a machine learning. And I will use it for later building Naive Bayes model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with simple Logistic regression and the TFIDF vectorizer for text feature engineering. Then eventually, I will build Naive bayes model to predict the sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_process(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub('[^a-z]+', ' ', text).strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.apply(lambda x: text_process(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(max_features=1000, lowercase=True, analyzer='word', stop_words= 'english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function to perform TfidfVectorizer for feature extraction and perform different machine learning models on those features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fit(X, y, feature_model,ml_model,coef_show=1):\n",
    "    \n",
    "    X_features = feature_model.fit_transform(X)\n",
    "    print('# features: {}'.format(X_features.shape[1]))\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_features, y, test_size = 0.2, random_state=43)\n",
    "    clf = ml_model.fit(X_train, y_train)\n",
    "    clf_pred = clf.predict(X_test)\n",
    "    accuracy = clf.score(X_test, y_test)\n",
    "    model_performance = classification_report(y_test, clf_pred)\n",
    "    print ('accuracy of the model: ', accuracy)\n",
    "    print('')\n",
    "    print(model_performance)\n",
    "    \n",
    "    if coef_show == 1: \n",
    "        w = feature_model.get_feature_names()\n",
    "        coef = clf.coef_.tolist()[0]\n",
    "        coeff_df = pd.DataFrame({'Word' : w, 'Coefficient' : coef})\n",
    "        coeff_df = coeff_df.sort_values(['Coefficient', 'Word'], ascending=[0, 1])\n",
    "        print('')\n",
    "        print('Top 10 positive features (variables)')\n",
    "        print(coeff_df.head(20).to_string(index=False))\n",
    "        print('')\n",
    "        print('Top 10 negative features (variables)')        \n",
    "        print(coeff_df.tail(20).to_string(index=False))\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the baseline model performance and will see how it performs. Also, look at the top ten text (word) features for both positive and negative coefficients. DummyClassifier() is classifier that makes predictions using simple rules. This classifier is useful as a simple baseline to compare with other(real) classifiers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# features: 1000\n",
      "accuracy of the model:  0.8185541574142611\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.10      0.10      0.10     10233\n",
      "           1       0.90      0.90      0.90     91092\n",
      "\n",
      "   micro avg       0.82      0.82      0.82    101325\n",
      "   macro avg       0.50      0.50      0.50    101325\n",
      "weighted avg       0.82      0.82      0.82    101325\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_fit(X, y, tfidf,DummyClassifier(),coef_show=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The baseline accuracy of the model is 82%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# features: 1000\n",
      "accuracy of the model:  0.9299481865284974\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.45      0.57     10233\n",
      "           1       0.94      0.98      0.96     91092\n",
      "\n",
      "   micro avg       0.93      0.93      0.93    101325\n",
      "   macro avg       0.85      0.72      0.76    101325\n",
      "weighted avg       0.92      0.93      0.92    101325\n",
      "\n",
      "\n",
      "Top 10 positive features (variables)\n",
      "Coefficient       Word\n",
      "   9.816006       love\n",
      "   9.326581    perfect\n",
      "   8.949654       easy\n",
      "   8.826484     highly\n",
      "   8.698433      great\n",
      "   7.347961    pleased\n",
      "   7.305565  excellent\n",
      "   7.082081       glad\n",
      "   6.487805  perfectly\n",
      "   5.846966       best\n",
      "   5.696329      works\n",
      "   5.628425      loves\n",
      "   5.444193    amazing\n",
      "   5.329842     sturdy\n",
      "   5.272612    awesome\n",
      "   5.259950      helps\n",
      "   5.133468  satisfied\n",
      "   5.082950       nice\n",
      "   5.080025      happy\n",
      "   5.049947      handy\n",
      "\n",
      "Top 10 negative features (variables)\n",
      "Coefficient           Word\n",
      "  -3.118974          money\n",
      "  -3.413680          cheap\n",
      "  -3.571113          ended\n",
      "  -3.593346        started\n",
      "  -3.885307           sent\n",
      "  -3.896732      difficult\n",
      "  -3.943025         months\n",
      "  -3.953707          broke\n",
      "  -4.040231         flimsy\n",
      "  -4.054343          tried\n",
      "  -4.076262          maybe\n",
      "  -4.128200         unless\n",
      "  -4.572406           idea\n",
      "  -5.497708        stopped\n",
      "  -5.701500          waste\n",
      "  -5.986354  unfortunately\n",
      "  -6.373174   disappointed\n",
      "  -7.329756           poor\n",
      "  -7.662009         return\n",
      "  -8.122328       returned\n"
     ]
    }
   ],
   "source": [
    "model_fit(X, y, tfidf,LogisticRegression(),coef_show=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy has improved from 82% to 93% with Logistic regression. When it comes to top 10 text features for both positive and negative coefficients do make sense. Now, I will add n-grams keyword argument to the TFIDF function and then I will run Logistic Regression model again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_n_grams = TfidfVectorizer(max_features=1000, lowercase=True, analyzer='word', stop_words= 'english', ngram_range=(1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# features: 1000\n",
      "accuracy of the model:  0.9298692326671602\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.46      0.57     10233\n",
      "           1       0.94      0.98      0.96     91092\n",
      "\n",
      "   micro avg       0.93      0.93      0.93    101325\n",
      "   macro avg       0.85      0.72      0.77    101325\n",
      "weighted avg       0.92      0.93      0.92    101325\n",
      "\n",
      "\n",
      "Top 10 positive features (variables)\n",
      "Coefficient              Word\n",
      "   9.849634              love\n",
      "   9.224187           perfect\n",
      "   8.078449             great\n",
      "   8.068077              easy\n",
      "   7.398137           pleased\n",
      "   7.350216         excellent\n",
      "   7.088916              glad\n",
      "   6.738402  highly recommend\n",
      "   6.462057         perfectly\n",
      "   5.876544              best\n",
      "   5.608813             works\n",
      "   5.597946             loves\n",
      "   5.496429           amazing\n",
      "   5.271526            sturdy\n",
      "   5.246291           awesome\n",
      "   5.231251             helps\n",
      "   5.207749              nice\n",
      "   5.163013         satisfied\n",
      "   5.063061             handy\n",
      "   4.802217            highly\n",
      "\n",
      "Top 10 negative features (variables)\n",
      "Coefficient           Word\n",
      "  -3.147987          money\n",
      "  -3.390621          cheap\n",
      "  -3.557843          ended\n",
      "  -3.572879        started\n",
      "  -3.851165           sent\n",
      "  -3.871301      difficult\n",
      "  -3.954785          broke\n",
      "  -3.978645          tried\n",
      "  -3.986299         months\n",
      "  -4.057157          maybe\n",
      "  -4.095623         flimsy\n",
      "  -4.110245         unless\n",
      "  -4.472905           idea\n",
      "  -5.448300        stopped\n",
      "  -5.682523          waste\n",
      "  -5.918379  unfortunately\n",
      "  -6.362950   disappointed\n",
      "  -7.166262           poor\n",
      "  -7.614519         return\n",
      "  -8.131678       returned\n"
     ]
    }
   ],
   "source": [
    "model_fit(X, y, tfidf_n_grams,LogisticRegression(),coef_show=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no big difference by introducing n-grams here in this model for Logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if there is any class imbalance here. If there is any class imbalance, then I will resolve that issue and will run the model again and will check the model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    455204\n",
       "0     51419\n",
       "Name: overall, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we check the above value counts, the target values are skewed towards positive side. In order to address this issue, resampling the data has to be performed. I will use oversampling to combat class imbalance. Smote is especially preferable since it's a well-made package with it's own pipeline function compatible with other python modules. Also, before applying SMOTE technique to balance the class, I would like lemmatize the review text and pass it to TFIDF vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_model['reviewText']\n",
    "X = X.apply(lambda x: text_process(x))\n",
    "X = X.apply(lambda x: ' '.join([Word(word).lemmatize() for word in x.split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_features = tfidf_n_grams.fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_features, y, test_size = 0.2, random_state=43)\n",
    "sm = SMOTE()\n",
    "X_train_smote, y_train_smote = sm.fit_sample(X_train, y_train)\n",
    "X_test_smote, y_test_smote = sm.fit_sample(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_balanced_model_fit(X_train_smote, y_train_smote, X_test_smote, y_test_smote, ml_model):\n",
    "    \n",
    "    clf = ml_model.fit(X_train_smote, y_train_smote)\n",
    "    clf_pred = clf.predict(X_test_smote)\n",
    "    accuracy = clf.score(X_test_smote, y_test_smote)\n",
    "    model_performance = classification_report(y_test_smote, clf_pred)\n",
    "    validation_pred_proba_grad = clf.predict_proba(X_test_smote)\n",
    "    roc_auc = roc_auc_score(y_test_smote, validation_pred_proba_grad[:,1])\n",
    "    \n",
    "    print ('accuracy of the model: ', accuracy)\n",
    "    print('')\n",
    "    print(model_performance)\n",
    "    print('')\n",
    "    print('ROC_AUC score: ', roc_auc)\n",
    "    \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy of the model:  0.8858351995784481\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.90      0.89     91092\n",
      "           1       0.90      0.87      0.88     91092\n",
      "\n",
      "   micro avg       0.89      0.89      0.89    182184\n",
      "   macro avg       0.89      0.89      0.89    182184\n",
      "weighted avg       0.89      0.89      0.89    182184\n",
      "\n",
      "\n",
      "ROC_AUC score:  0.9508239968268111\n"
     ]
    }
   ],
   "source": [
    "class_balanced_model_fit(X_train_smote, y_train_smote, X_test_smote, y_test_smote, LogisticRegression())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the accuracy is 89% and auc of the ROC curve is 95%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now apply SVM model to classify and see the model performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class_balanced_model_fit(X_train_smote, y_train_smote, X_test_smote, y_test_smote, svm.SVC())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support Vector Machines are powerful tools, but their compute and storage requirements increase rapidly with the number of training vectors. The core of an SVM is quadratic programming problem (QP), separating support vectors from the rest of the training data. Also, note that for the linear case, the algorithm used in LinearSVC by the liblinear implementation is much more efficient than its libsvm-based SVC counterpart and can scale almost linearly to millions of samples and/or features. SVM - training with nonlinear-kernels, which is default in sklearn's SVC, is complexity-wise approximately: 0(n_samples^2 * n_features). This applies to to the SMO-algorithm used within libsvm, which is the core-solver in sklearn for this type of problem. This changes much when no kernels are used and one uses sklearn.svm.LinearSVC (based on liblinear) or sklearn.linear_model.SGDClassifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
